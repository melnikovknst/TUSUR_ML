{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ufbmHdl_P7LA5QfI1oZ6S_Nh4kUoYr2A","timestamp":1731425554968}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Введение"],"metadata":{"id":"-nsUOodn-8GG"}},{"cell_type":"markdown","source":["На этой лекции мы рассмотрим программные средства для анализа данных,  визуализации, для разработки моделей машинного обучения и нейронных сетей на языке Python. Также помимо непосредственной работы с данными важно помнить, что и данные, и код для обработки данных, и само обучение моделей вместе с получаемыми метриками необходимо сохранять для упрощения дальнейшего сравнения между версиями, для чего также существует ряд инструментов.\n","\n","Начнем с базовых терминов, затем рассмотрим среду Jupyter Notebook и другие инструменты.\n"],"metadata":{"id":"xeycSQ3MahNm"}},{"cell_type":"markdown","source":["# Понятие датасета и признаков"],"metadata":{"id":"AUKs6jNoES2v"}},{"cell_type":"markdown","source":["**Данные** — это информация, которая записана или зафиксирована каким-либо образом, чтобы ее можно было обработать или использовать. Данные могут быть самых разных типов: числовые, текстовые, изображения, звуки и т.д. В контексте машинного обучения данные являются исходным материалом, на основе которого строятся и обучаются модели.\n","\n","**Датасет**, в свою очередь, является организованным набоом данных, который предназначен для анализа и использования в моделях машинного обучения. Датасет обычно состоит из строк и столбцов, где каждая строка представляет собой отдельный объект или запись, а каждый столбец — это определенная характеристика или атрибут этого объекта. Датасеты могут быть как структурированными (например, таблицы в базе данных), так и неструктурированными (например, тексты, изображения).\n","\n","**Признаки** — это конкретные характеристики или переменные, которые используются для описания объектов в датасете. В контексте машинного обучения признаки являются входными данными для модели, и их выбор и подготовка играют ключевую роль в эффективности модели. Признаки, например, в таблицах могут быть различных типов: числовые, категориальные, бинарные и т.д.\n","\n","В первом семестре мы в основном будем работать именно с табличными датасетами, и по мере усложнения программы — с изображениями, текстом."],"metadata":{"id":"T8WjTzhGQZbT"}},{"cell_type":"markdown","source":["# Где писать код: среда Jupyter и Google Colaboratory"],"metadata":{"id":"zZRSDTy0_0If"}},{"cell_type":"markdown","source":["Однако для работы с данными нужно специальное пространство, среда.\n","\n","**Jupyter Notebook** — это мощная среда для разработки моделей машинного обучения, особенно популярная среди исследователей, аналитиков данных и инженеров машинного обучения. Она предоставляет удобный интерфейс для написания и исполнения кода, визуализации данных и документирования результатов.\n","\n","Так сложилось, что классические среды разработки (например, VSCode, PyCharm) и работа в Python-скриптах не очень удобна при работе с анализом данных и обучением, так как приходится либо запускать весь скрипт заново, либо создавать много отдельных скриптов и работать с ними, что, согласитесь, не очень удобно на этапе экспериментов. Поэтому профессиональным стандартом является работа в Jupyter.\n","\n","* Jupyter Notebook позволяет пользователям **выполнять код по ячейкам**, что делает процесс исследования и разработки чрезвычайно интерактивным, в отличие от классической разработки в скриптах. Это особенно полезно в машинном обучении, где часто требуется быстрая итерация и визуализация данных, небольшое добавление кода, проверка данных и изменение кода. Весь код выполняется в отдельных ячейках (cells), но все ячейки связаны между собой глобальным контекстом - результат выполнения одной ячейки будет сохраняться, и при запуске следующей ячейки значение переменной будет использоваться из предыдущей запущенной ячейки. Это позволяет разработчикам писать мини-скрипты в разных ячейках, запускать их независимо и в произвольном порядке. Встроенные возможности визуализации, такие как Matplotlib и Seaborn, позволяют с легкостью создавать графики и диаграммы прямо в ноутбуке, что упрощает анализ и интерпретацию данных.\n","\n","* Jupyter Notebook позволяет **легко комбинировать код, результаты, графики и текстовое описание** в одном документе. Это делает его идеальным для документирования процесса исследования и обеспечения воспроизводимости результатов. Исследователи и разработчики могут делиться своими ноутбуками с другими, что упрощает обмен знаниями и сотрудничество. Такой ноутбук достаточно просто скачать и запустить, установку всех библиотек можно прописать в коде самого ноутбука - при запуске ячейки скачаются все необходимые зависимости.\n","\n","* Jupyter Notebook имеет **интуитивно понятный интерфейс**, который легко освоить даже новичкам. Он работает в веб-браузере - по умолчанию на localhost, что делает его доступным с различных устройств и платформ. Кроме того, существует множество облачных сервисов, таких как Google Colab и Microsoft Azure Notebooks, которые предоставляют бесплатный доступ к Jupyter Notebook с возможностью использования мощных вычислительных ресурсов.\n","\n","* Jupyter Notebook хорошо **интегрируется с популярными библиотеками машинного обучения**, такими как TensorFlow, PyTorch, Scikit-learn и другими — это позволяет разработчикам быстро прототипировать и тестировать модели, а также легко отлаживать код благодаря интерактивному режиму выполнения. Также сам Jupyter поставляется с уже стартовым набором библиотек для аналитики данных - pandas, numpy, scipy, matplotlib, seaborn и другие, их дополнительно скачивать не надо."],"metadata":{"id":"DDCHqwT8TE6t"}},{"cell_type":"markdown","source":["**Ядро (kernel)** — вычислительный движок, который выполняет код, содержащийся в документе ноутбука.\n","\n","**Ячейка (cell)** — контейнер для кода, который будет выполняться, или для сопутствующей информации в разметке.\n","\n","Jupyter Notebook предоставляет удобный способ экспериментировать с кодом на Python, чередуя эксперименты с заметками и документацией. Таким образом, проводить необходимые эксперименты и анализ можно не прибегая к командной строке, а полученный файл можно легко опубликовать и поделиться им с другими людьми.\n","\n","Jupyter Notebook состоит из нескольких «ячеек», расположенных на странице сверху вниз. Ячейки могут содержать текст или код в них. Вы можете изменить тип ячейки, используя меню Cell в верхней части страницы; перейдите в Cell > Cell type и выберите либо Code для кода Python, либо Markdown для текста. Вы также можете изменить это для текущей ячейки, используя выпадающее меню на панели инструментов."],"metadata":{"id":"B1m5UrRDoVan"}},{"cell_type":"markdown","source":["## Установка Jupyter Notebook локально"],"metadata":{"id":"1Ih6XPYsYMR9"}},{"cell_type":"markdown","source":["Установка Jupyter Notebook на локальный компьютер достаточно проста и включает несколько шагов:\n","\n","**Шаг 1:** Установка Python\n","\n","Jupyter Notebook требует Python. Если у вас еще не установлен Python, вы можете скачать его с официального сайта python.org. Рекомендуется использовать последнюю стабильную версию.\n","\n","**Шаг 2:** Установка Jupyter Notebook\n","\n","Существует несколько способов установки Jupyter Notebook. Самый простой — использовать пакетный менеджер pip.\n","\n","Откройте терминал или командную строку и выполните следующую команду:\n","```code\n","pip install notebook\n","```\n","\n","**Шаг 3:** Запуск Jupyter Notebook\n","\n","После установки вы можете запустить Jupyter Notebook с помощью следующей команды:\n","```code\n","jupyter notebook\n","```\n","\n","Эта команда запустит сервер Jupyter Notebook и откроет веб-браузер по умолчанию, где вы сможете видеть интерфейс Jupyter Notebook.\n","\n","**Note**: Для лучшей изоляции зависимостей рекомендуется использовать **виртуальное окружение**. Вы можете создать виртуальное окружение с помощью venv или conda.\n","\n","Пример с venv:\n","```code\n","python -m venv myenv\n","source myenv/bin/activate  # Для Linux/MacOS\n","myenv\\Scripts\\activate  # Для Windows\n","```\n","После активации виртуального окружения выполните установку Jupyter Notebook, как описано выше."],"metadata":{"id":"fc6CMbczpnB_"}},{"cell_type":"markdown","source":["## Работа с Jupyter Notebook в среде Google Colab"],"metadata":{"id":"JBFOI0tPYQZw"}},{"cell_type":"markdown","source":["В этой среде будет **99% работы**, особенно при обучении глубоких моделей, и **рекомендуем** пользоваться ей.\n","\n","Google **Colab** (сокращение от Google Colaboratory) — это бесплатная облачная служба, предоставляемая Google, которая позволяет пользователям создавать, редактировать и запускать Jupyter Notebook прямо в браузере. Colab особенно популярен среди специалистов по данным, исследователей и студентов, так как он предлагает доступ к мощным вычислительным ресурсам, включая GPU и TPU, без необходимости установки какого-либо программного обеспечения на локальный компьютер."],"metadata":{"id":"ftzfuM-sYVjn"}},{"cell_type":"markdown","source":["Основные возможности Google Colab:\n","* **Облачное исполнение** кода: Google Colab работает полностью в облаке, поэтому нет необходимости устанавливать Python и зависимости на локальном компьютере. Все вычисления выполняются на серверах Google.\n","\n","* Поддержка **GPU и TPU**: Colab позволяет бесплатно использовать GPU и TPU для ускорения вычислений. Это особенно полезно для задач машинного обучения, требующих больших вычислительных ресурсов. Однако сессии с GPU/TPU ограничены 4-мя часами, а на CPU - 12 часов интерактивной работы. При работе с GPU/TPU следующая сессия доступна примерно через 10-12 часов.\n","\n","* **Поддержка Jupyter Notebook**: Colab использует формат Jupyter Notebook, что делает его удобным для написания и выполнения Python-кода, создания отчетов с визуализацией данных, пояснительных текстов и многого другого.\n","\n","* **Совместная работа**: Google Colab позволяет нескольким пользователям работать над одним и тем же ноутбуком. Это упрощает совместную работу, например, над учебными проектами или исследовательскими задачами. Однако на данный момент одновременная работа не поддерживается - на одной из копий будут ошибки сохранения и разные версии.\n","\n","* Автоматическое сохранение: Все изменения сохраняются автоматически на Google Drive, что позволяет легко управлять версиями и восстанавливать изменения при необходимости.\n","\n","* Доступ к данным: В Colab легко подключаться к различным источникам данных, включая **Google Drive, Google Sheets**, а также загружать данные напрямую из интернета."],"metadata":{"id":"-DcVd7DfruRN"}},{"cell_type":"markdown","source":["Как работать с Colab:\n","1. **Войдите в Google аккаунт**: Перейдите на сайт Google Colab и войдите в свой Google аккаунт.\n","\n","2. **Создайте новый ноутбук**: Нажмите на кнопку \"Новый ноутбук\" или откройте существующий ноутбук из Google Диска.\n","\n","3. **Начните работу**: Вы можете начать писать код, добавлять текстовые блоки, вставлять изображения и многое другое. Для выполнения кода просто нажмите на кнопку \"Выполнить\" (треугольник) рядом с ячейкой.\n","\n","4. При необходимости, используйте GPU/TPU: Для использования GPU или TPU перейдите в меню \"Подключиться\" -> треугольник раскрывающегося меню ->\"Сменить среду выполнения\" и выберите нужный тип аппаратного ускорителя."],"metadata":{"id":"CE2OmJxJs3ry"}},{"cell_type":"markdown","source":["# GPU, TPU vs CPU для глубокого обучения"],"metadata":{"id":"xKdiLbQItZjn"}},{"cell_type":"markdown","source":["Для задач глубокого обучения, таких как обучение нейронных сетей, требуются значительные вычислительные ресурсы из-за объема данных и сложности вычислений. Хотя **центральные процессоры** (**CPU**) могут выполнять эти задачи, они часто не справляются с ними эффективно. В реальных сценариях глубокого обучения, таких как компьютерное зрение, обработка естественного языка и генеративные модели, обработка огромных наборов данных и сложных моделей на CPU может занять дни или даже недели. GPU и TPU могут выполнить ту же задачу за часы или дни, что делает их критически важными для разработки и внедрения современных AI-технологий.\n","\n","**GPU** (Graphics Processing Unit) и **TPU** (Tensor Processing Unit) — это специализированные процессоры, разработанные для выполнения определённых типов вычислений, особенно тех, которые требуются в графике и машинном обучении\n","\n","**GPU**, или **графический процессор**, изначально был разработан для обработки графики и выполнения операций, связанных с отображением изображений, рендерингом 3D-графики и видео. Со временем его возможности стали активно использоваться и в других областях, особенно в научных вычислениях и машинном обучении.\n","\n","Основными операциями в глубоких нейронных сетях являются операции над матрицами и тензорами (многомерными массивами данных). Например, при умножении матрицы весов на вектор признаков входных данных выполняется множество однотипных операций. GPU, в отличие от универсальных CPU, **изначально проектировались для обработки многомерных изображений**, для которой как раз нужны матричные вычисления. Они исполняются с помощью вычислительных блоков, которых в GPU гораздо больше, чем в CPU.\n","\n","**Графические процессоры состоят из тысяч небольших ядер**, способных выполнять множество простых вычислений параллельно. Например, современные GPU могут иметь от нескольких сотен до нескольких тысяч ядер (например, у NVIDIA A100 их 6912). В отличие от CPU, которые обычно имеют ограниченное число мощных ядер (4-16), GPU позволяют выполнять одновременно тысячи операций, что критически важно для обучения глубоких нейронных сетей, где требуется многократное выполнение одних и тех же операций для больших объемов данных.\n","\n","> Библиотеки и фреймворки, такие как TensorFlow, PyTorch, и Keras, активно используют возможности GPU для ускорения обучения моделей. Они предоставляют оптимизированные функции и алгоритмы, специально разработанные для выполнения на GPU. Например, NVIDIA разработала **CUDA** (Compute Unified Device Architecture), которая позволяет программистам использовать GPU для общих вычислений, не связанных с графикой. CUDA предоставляет инструменты и библиотеки, специально оптимизированные для выполнения задач глубокого обучения.\n","\n","Одним из преимуществ GPU является их возможность **масштабирования**. В сложных задачах, таких как обучение очень больших моделей или работа с огромными наборами данных, можно использовать несколько GPU одновременно. Это значительно ускоряет процесс обучения, позволяя распределять задачи по нескольким устройствам и выполняя их параллельно. В современных центрах обработки данных и облачных платформах используются **кластеры GPU**, что позволяет обучать модели на ещё более масштабных наборах данных и с большей эффективностью.\n","\n","**TPU** — это специализированный процессор, разработанный Google специально для задач машинного обучения, в частности для работы с нейронными сетями. TPU оптимизирован для выполнения операций с тензорами, которые лежат в основе многих алгоритмов машинного обучения. В отличие от CPU и GPU, которые имеют универсальную архитектуру для выполнения широкого спектра задач, TPU сосредоточены на специфических вычислениях, которые чаще всего встречаются в машинном обучении. Тенхорные процессоры обеспечивают высокую производительность за счет оптимизации вычислений на уровне аппаратного обеспечения. Они способны выполнять триллионы операций с плавающей запятой в секунду (FLOPS), что значительно ускоряет процесс обучения моделей.\n","\n","> Google разработал TPU для использования в связке с TensorFlow — популярным фреймворком для машинного обучения. TensorFlow включает в себя оптимизации, специально разработанные для работы с TPU, что позволяет пользователям максимально эффективно использовать возможности этого процессора.\n","\n","\n","Резюмируя:\n","> **CPU**: Несмотря на универсальность и способность к выполнению сложных логических операций, их производительность в задачах глубокого обучения ограничена.\n","\n","> **GPU**: Они могут выполнять огромное количество простых операций одновременно, что делает их гораздо быстрее для типичных задач глубокого обучения, таких как обучение нейронных сетей.\n","\n","> **TPU**: Эти устройства еще быстрее, чем GPU, в специфических задачах, связанных с машинным обучением, поскольку их архитектура спроектирована специально для этого."],"metadata":{"id":"UZ2KBPMkuFQT"}},{"cell_type":"markdown","source":["# Как данные отображаются в многомерном пространстве"],"metadata":{"id":"-WtMU5_9_LXs"}},{"cell_type":"markdown","source":["# Понятие тензора"],"metadata":{"id":"mY-_bKZz_teI"}},{"cell_type":"markdown","source":["**Тензор** — это математическая структура, которая играет ключевую роль в машинном обучении, особенно в области обработки данных. Чтобы понять, что такое тензор, важно рассмотреть его как обобщение понятий скаляров, векторов и матриц. Тензоры являются многомерными массивами чисел, которые могут содержать данные различной природы: изображения, текст, аудио и другие.\n","\n","Современные системы машинного обучения используют тензоры в качестве основной структуры данных.\n","\n","Начнем с того, что тензор — это структура, которая хранит данные в виде чисел, организованных в многомерные массивы. Например:\n","\n","* Одномерные тензоры можно представить как одномерные массивы чисел. Тензор первого порядка имеет только одно измерение. В компьютерных науках это можно назвать вектором или упорядоченным набором чисел.  Векторы используются для представления данных, которые организованы в линейной последовательности, например, координаты точки на плоскости (x,y).\n","\n","* Двумерные тензоры — это двумерные массивы чисел, организованные в строки и столбцы, то есть матрицы. Матрицы широко применяются в задачах линейной алгебры, в том числе для представления изображений в цифровом формате, где каждый элемент матрицы соответствует интенсивности пикселя.\n","\n","* Тензоры более высоких порядков (трехмерные, четырехмерные и выше) представляют собой многомерные массивы чисел, которые могут быть использованы для хранения данных более сложной структуры, таких как серии изображений или временные ряды данных.\n","\n","Если упаковать матрицы в новый массив, получится трехмерный тензор, который можно представить как числовой куб. Упаковав трехмерные тензоры в массив, вы получите четырехмерный тензор и т.д.\n","В глубоком обучении чаще всего используются тензоры от нулевого ранга до четырехмерных, но иногда, например, при обработке видеоданных, дело может дойти и до пятимерных тензоров.\n"],"metadata":{"id":"Z_ia0vOtJ8uW"}},{"cell_type":"markdown","source":["https://forum.huawei.com/enterprise/api/file/v1/small/thread/667494826096070656.png?appid=esc_ru"],"metadata":{"id":"UejJcHZpNTtI"}},{"cell_type":"markdown","source":["## Разные виды данных: табличные, аудио, изображения, видео, графы"],"metadata":{"id":"yXpmiouJGgr8"}},{"cell_type":"markdown","source":["Векторные данные - двумерные тензоры с формой (образцы, признаки).\n","\n","Временные ряды или последовательности - трехмерные тензоры с формой\n","(образцы, метки времени, признаки);\n","\n","Изображения - четырехмерные тензоры с формой (образцы, высота, ширина,\n","цвет) или с формой (образцы, цвет, высота, ширина).\n","\n","Видео - пятимерные тензоры с формой (образцы, кадры, высота, ширина,\n","цвет) или с формой (образцы, кадры, цвет, высота, ширина)."],"metadata":{"id":"bNN-dKXoRU3R"}},{"cell_type":"markdown","source":["## Размерность данных"],"metadata":{"id":"7gfP7ckvOYbi"}},{"cell_type":"markdown","source":["У данных также есть понятие «размерности». Рассмотрим пример табличных данных. Размерность соответствует количеству столбцов в таблице. Каждый столбец представляет собой отдельный признак (или переменную), по которому измеряются данные. Например, если у вас есть таблица с шестью столбцами, где записаны такие признаки, как возраст, рост, вес, пол и доход, то размерность данных составляет 6.\n","\n","**Размерность данных** описывает количество измерений или факторов, по которым может варьироваться объект или явление. Это понятие обычно используется в контексте анализа данных и машинного обучения.\n","\n","> **Тензор** описывает структуру данных на уровне математического объекта, представляя данные в виде многомерного массива.\n","\n","> **Размерность** данных касается именно количества признаков или переменных в наборе данных."],"metadata":{"id":"h7Gb2HyaOvAC"}},{"cell_type":"markdown","source":["# Источники данных для разработки"],"metadata":{"id":"Aw1SLpkebL2n"}},{"cell_type":"markdown","source":["В машинном обучении источники данных играют ключевую роль, так как модели обучаются на данных и от их качества и объема во многом зависит эффективность и точность предсказаний. Существуют различные типы источников данных, которые могут использоваться в машинном обучении, включая базы данных, хранилища данных, озера данных и другие."],"metadata":{"id":"5Mm7z6F93nXZ"}},{"cell_type":"markdown","source":["## Объёмы данных"],"metadata":{"id":"l2ROYl1KcFcY"}},{"cell_type":"markdown","source":["Чем сложнее модель и больше ее параметров, тем больше данных требуется для ее обучения. Например, простым моделям линейной регрессии может потребоваться несколько сотен записей, в то время как глубокие нейронные сети, такие как трансформеры или другие архитектуры LLM (Large Language Model), могут требовать миллионы или даже миллиарды записей.\n","\n","Общие рекомендации:\n","* Малые модели (линейная регрессия, решающие деревья): от сотен до 10,000 записей.\n","* Средние модели (сложные ансамбли, неглубокие нейронные сети): от 10,000 до 100,000 записей.\n","* Глубокие модели (CNN, RNN, трансформеры): от 50.000 до миллионов записей.\n","\n","Однако и это деление весьма условное, так как многое зависит от исходной задачи.\n","\n","Если данных недостаточно для сложных моделей, возникает риск переобучения (overfitting), когда модель слишком точно запоминает тренировочные данные, но плохо работает на новых данных. Для борьбы с этим применяют методы регуляризации или увеличивают объем данных, например, за счет генерации дополнительных данных или добавления данных из других внешних источников.\n","\n"," А какие есть варианты для получения и хранения данных?"],"metadata":{"id":"QjpUCMwx4v5Z"}},{"cell_type":"markdown","source":["## Реляционные БД"],"metadata":{"id":"Y3gkixoYbOre"}},{"cell_type":"markdown","source":["**Реляционные базы данных** (RDBMS) — это традиционные базы данных, такие как MySQL, PostgreSQL, и Oracle, которые организуют данные в таблицы, связанные между собой. Реляционные базы данных часто используются для хранения структурированных данных, таких как транзакции, записи клиентов и другие бизнес-данные. В таких БД данные организованы в таблицы с четко определенными схемами. Каждая таблица состоит из строк и столбцов, где строки представляют записи, а столбцы — атрибуты данных.\n","\n","> Плюсы: Структурированные данные, поддержка сложных запросов на SQL.\n","\n","> Минусы: Ограниченная масштабируемость и сложность обработки больших объемов данных.\n","\n","> Примеры использования: Банковские данные, управление запасами, учетные записи клиентов."],"metadata":{"id":"tpyXwa6u3vS5"}},{"cell_type":"markdown","source":["## Нереляционные БД"],"metadata":{"id":"ci4j4FEN4QvP"}},{"cell_type":"markdown","source":["**Нереляционные** базы данных (NoSQL — \"Not only SQL\") — это тип баз данных, которые отличаются от традиционных реляционных баз данных. Основное отличие состоит в том, что данные в NoSQL хранятся не в виде таблиц с жесткими структурами (строки и столбцы), а в гибких, менее строгих форматах, что делает их более подходящими для хранения большого объема разнородных данных.\n","\n","Нереляционные базы — это общее название семейства. К ним относятся:\n","\n","* **Документоориентированные** базы данных: эти базы данных хранят данные в виде документов, обычно в формате JSON, BSON, XML или других. Каждый документ может содержать сложные иерархические структуры (вложенные документы, массивы и т.д.), что делает их гибкими для хранения данных с различной структурой. Яркий пример — MongoDB.\n","* **Графовые** базы данных: эти базы данных используются для хранения данных в виде вершин (узлов) и ребер (связей). Они полезны для работы с данными, где важны отношения между объектами, например, социальные сети. Пример базы — Neo4j.\n","* Базы данных формата **\"ключ-значение\"**: эти базы данных хранят данные в виде пар \"ключ-значение\". Они очень быстрые и масштабируемые, но обычно не поддерживают сложные запросы, такие как поиск по значениям. Пример — Redis.\n","* **Колоночные** базы данных: это тип нереляционных баз данных, которые организуют и хранят данные по столбцам вместо строк, как это делают традиционные реляционные базы данных. Этот подход оптимизирован для работы с большими объемами данных и позволяет эффективно обрабатывать запросы на чтение и запись, особенно когда нужно работать с конкретными столбцами данных, а не с целыми строками. Пример — Cassandra."],"metadata":{"id":"8TYqoV82BI1x"}},{"cell_type":"markdown","source":["Такие базы данных, как MongoDB, Cassandra, и Redis, предназначены для хранения неструктурированных или полуструктурированных данных. В отличие от RDBMS, NoSQL базы данных поддерживают гибкие схемы или вообще не требуют схемы, что позволяет легко хранить данные, которые могут изменяться по структуре. Такие БД часто используются для хранения больших объемов данных.\n","\n","> Плюсы: Высокая масштабируемость, гибкость в работе с разнородными данными.\n","\n","> Минусы: Отсутствие поддержки сложных транзакций, как в реляционных базах.\n","\n","**Транзакция** — это последовательность операций с базой данных, которые выполняются как единое целое. Все операции транзакции должны быть выполнены успешно, или, в случае ошибки, все изменения отменяются, и база данных возвращается к состоянию до начала транзакции. Сложные транзакции включают множество операций, которые затрагивают несколько таблиц или даже разные базы данных. Эти операции могут быть взаимосвязанными и должны быть выполнены в строгом порядке. Такие транзакции часто применяются в сценариях, где данные должны быть обновлены в нескольких местах одновременно.\n","\n","> Примеры использования: Лог-файлы, социальные медиа, данные IoT.\n"],"metadata":{"id":"E1qQACuE4TQg"}},{"cell_type":"markdown","source":["## Data Warehouse"],"metadata":{"id":"evTAN5kRbRVk"}},{"cell_type":"markdown","source":["**Хранилища данных (Data Warehouse, DWH)** — это централизованная система, предназначенная для хранения, интеграции и анализа больших объемов данных из различных источников. Основная цель хранилища данных — объединить данные из разных систем (например, баз данных, внешних источников данных, приложений) для анализа и поддержки принятия решений на уровне бизнеса. Данные в хранилища часто поступают через процессы ETL (Extract, Transform, Load), которые включают извлечение данных из разных источников, их трансформацию и загрузку в хранилище данных. Data Warehouse предназначены для обработки сложных аналитических запросов, таких как агрегаты, фильтрация, сводки и другие виды аналитики. Эти запросы, как правило, требуют сканирования больших объемов данных, поэтому хранилища данных оптимизированы для быстрых операций чтения, а не записи.\n","\n","DWH нередко проектируют на сторонних сервисах: Amazon Redshift, Google BigQuery, Snowflake.\n","\n","Стоит понимать, что построение и поддержка хранилища данных может быть дорогостоящей задачей из-за необходимости хранения колоссального количества данных, аппаратной и программной архитектуры для обеспечения работы хранилища (или их аренды).\n","\n","> Плюсы: Высокая производительность для анализа данных, возможность объединять данные из разных источников.\n","\n","> Минусы: Сложность настройки и управления, фокус на структурированных данных.\n","\n","> Примеры использования: Аналитика продаж, управление цепочками поставок, отчетность."],"metadata":{"id":"sHRpXh3P6_kj"}},{"cell_type":"markdown","source":["## Data Lake"],"metadata":{"id":"42QGDfwf6-Eh"}},{"cell_type":"markdown","source":["**Озера данных (Data Lakes)** — это хранилища, которые могут содержать структурированные, полуструктурированные и неструктурированные данные, включая текстовые файлы, изображения, видео, логи, и другие типы данных. В отличие от хранилищ данных, озера предназначены для хранения данных в их исходном (сыром) виде, без необходимости предварительной обработки или структурирования. Из-за отсутствия жесткой схемы и структуры данных озера данных могут превратиться в \"болота данных\" (data swamp), если не обеспечена правильная организация и управление данными. Озера данных идеально подходят для хранения огромных объемов разнородных данных для разнообразных и комплексных задач, которые могут использоваться для обучения моделей машинного обучения, особенно в задачах глубокого обучения, где требуются большие и разнообразные данные.\n","\n","На базе каких технологий можно строить озёра: Apache Hadoop (потихоньку устаревает), Amazon S3, Azure Data Lake.\n","\n","> Плюсы: Хранение огромных объемов разнородных данных в сыром виде, гибкость в обработке данных.\n","\n","> Минусы: Необходимость сложных процессов для извлечения и анализа данных, риски связанные с качеством данных (так называемый \"data swamp\").\n","\n","> Примеры использования: Хранение данных из социальных сетей, данных с датчиков, медиа-файлы."],"metadata":{"id":"fWL0RGVH8x-q"}},{"cell_type":"markdown","source":["# Почему Python\n","\n","**Python стал де-факто стандартом в машинном обучении** благодаря сочетанию факторов, которые делают его идеальным выбором для задач в этой области. Одним из главных преимуществ Python является его **простота** и читаемость кода. Это высокоуровневый язык, с интуитивно понятным синтаксисом, который облегчает обучение и использование, что особенно важно в контексте сложных задач машинного обучения, где важно фокусироваться на алгоритмах и данных, а не на синтаксических деталях.\n","\n","Ключевым фактором стала **мощная и обширная экосистема библиотек и фреймворков**, созданная сообществом. Библиотеки такие как NumPy и SciPy обеспечивают необходимые инструменты для численных вычислений, а pandas предоставляет мощные средства для работы с данными. В области машинного обучения и глубокого обучения Python поддерживает такие фреймворки как TensorFlow, PyTorch, Keras и Scikit-learn. Эти инструменты обеспечивают удобство и гибкость в реализации и тестировании моделей, а также оптимизированы для работы с большими объемами данных и вычислений на GPU и TPU.\n","\n","Python также обладает **сильной поддержкой со стороны научного и академического сообщества**. Многие исследовательские работы и научные статьи в области машинного обучения сопровождаются кодом на Python, что способствует его распространению в научных кругах. Университеты и курсы по всему миру преподают машинное обучение и анализ данных на Python, что делает его первым языком, с которым сталкиваются новички в этой области.\n","\n","Еще одним важным аспектом является активное сообщество разработчиков и исследователей, которое **постоянно развивает инструменты и делится наработками в открытом доступе**. Это приводит к быстрому распространению новых методов и подходов, а также к улучшению существующих инструментов. Наличие большого числа учебных материалов, примеров кода и документации также способствует популярности Python в машинном обучении.\n","\n","Не менее важно и то, что Python хорошо интегрируется с другими языками и системами. Например, для критичных по производительности задач можно использовать **Cython или Numba** для ускорения кода, или интегрировать C/C++ библиотеки для выполнения вычислительно тяжелых операций. Такая гибкость делает Python привлекательным для широкого спектра задач, выходящих за рамки только машинного обучения."],"metadata":{"id":"NYU0UvjGeWUR"}},{"cell_type":"markdown","source":["# R — не конкурент, но дополнение для анализа данных"],"metadata":{"id":"SttZZSm__V-I"}},{"cell_type":"markdown","source":["**R** — это язык программирования и программная среда, специально разработанная для статистических вычислений и графического анализа данных. Он особенно популярен среди статистиков, исследователей и аналитиков данных благодаря своей мощной функциональности и возможностям для выполнения сложных статистических анализов.\n","\n","На протяжении многих лет вокруг R сформировалась огромная экосистема пакетов (более 15 000 пакетов), которая охватывает **практически все аспекты анализа данных**: от базовой статистики до сложного машинного обучения и визуализации данных.\n","\n","Одной из главных особенностей R является его **мощь в области статистики**. R предлагает богатый набор встроенных функций для выполнения статистических тестов, анализа временных рядов, регрессионного анализа, кластеризации и других методов. Благодаря этому, R широко используется в академических исследованиях, где требуется точный и воспроизводимый статистический анализ.\n","\n","Важным аспектом работы с данными в R является его подход к визуализации. **R известен своими возможностями в создании качественных графиков и визуализаций данных.** Пакеты, такие как ggplot2, позволяют создавать сложные и настраиваемые графики с минимальными усилиями. Это делает R отличным выбором для задач, требующих глубокого анализа данных и их представления в виде графиков и диаграмм. Гибкость и мощность графических возможностей R позволяют визуализировать данные на высоком уровне, что особенно ценно при подготовке отчетов или публикаций.\n","\n","Однако у R есть и свои недостатки. Основной из них — это производительность. R не так быстр, как языки, такие как Python или C++, особенно при обработке больших объемов данных или выполнении вычислительно интенсивных задач."],"metadata":{"id":"ruED1GSZ_jmo"}},{"cell_type":"markdown","source":["# Популярные библиотеки Python: обзор"],"metadata":{"id":"3WosLPKlAXh1"}},{"cell_type":"markdown","source":["## Numpy"],"metadata":{"id":"Pl7_JBoRAmWg"}},{"cell_type":"markdown","source":["NumPy — это мощная библиотека для языка программирования Python, которая предоставляет инструменты для работы с многомерными массивами и матричными данными, а также большой набор математических функций для работы с этими массивами. В машинном обучении NumPy играет ключевую роль **благодаря своим функциям для эффективной обработки и манипуляции данными**.  NumPy обеспечивает высокопроизводительные операции над массивами данных. Так как он реализован на языке C, это позволяет выполнять вычисления значительно быстрее, чем с использованием стандартных списков Python, что делает его идеальным для работы с большими наборами данных, которые часто встречаются в задачах машинного обучения и анализа данных. Однако часто придется работать с ней опосредованно — структуры и функции из этой библиотеки под капотом используют другие популярные инструменты из-за оптимизированности и ускорения вычислений."],"metadata":{"id":"uTPIcqpldEea"}},{"cell_type":"markdown","source":["## Pandas"],"metadata":{"id":"mxmXiPIrAu4q"}},{"cell_type":"markdown","source":["Pandas был разработан для обеспечения высокой производительности и простоты использования при работе с табличными данными (например, электронными таблицами или базами данных). **Это профессиональный стандарт и основная библиотека по анализу данных, с которой будем работать.**\n","\n","Pandas позволяет получать табличные данные из разных источников: CSV и Excel-файлы, реляционные базы данных, файловые хранилища, крупные хабы (например, датасеты с HuggingFace)."],"metadata":{"id":"O57QSi8ifNXc"}},{"cell_type":"markdown","source":["## Scikit-Learn"],"metadata":{"id":"unHtckeVYi4V"}},{"cell_type":"markdown","source":["Scikit-Learn, или сокращенно sklearn, предоставляет простой и унифицированный интерфейс для множества алгоритмов и инструментов как подготовки данных, так и алгоритмов классического ML, что делает ее идеальной для начинающих и опытных специалистов в области машинного обучения.\n","\n","Популярной делает эту библиотеку упор на классическое машинное обучение:\n","\n","* широкий спектр алгоритмов для классификации, регрессии, кластеризации и уменьшения размерности. Сюда входят популярные методы, такие как линейная регрессия, метод опорных векторов (SVM), случайный лес, k-ближайших соседей (k-NN) и многие другие. В дальнейших практиках реализации алгоритмов мы будем использовать из этой библиотеки.\n","\n","* множество методов для оценки производительности моделей, включая метрики (например, accuracy, precision, recall, F1-score), кросс-валидацию, а также инструменты для настройки гиперпараметров и выбора модели.\n","\n","Не пугайтесь, если какие-то слова здесь вам кажутся страшными и незнакомыми - мы их рассмотрим дальше при работе с курсом. Scikit-learn известен отличной документацией, что делает его доступным для широкого круга пользователей. Библиотека сопровождается множеством примеров и учебных материалов, которые помогают пользователям быстро начать работу и изучить различные аспекты машинного обучения. Так, помимо гайдлайнов по разным аспектам машинного обучения у Scikit-Learn есть собственный курс по машинному обучению и работе с библиотекой."],"metadata":{"id":"FRSfMZFWd4dZ"}},{"cell_type":"markdown","source":["## Spark"],"metadata":{"id":"P_yO3mrdGoET"}},{"cell_type":"markdown","source":["**Apache Spark** — это распределенная вычислительная платформа с открытым исходным кодом, предназначенная для обработки больших объемов данных. Spark был разработан для обеспечения **высокой производительности при обработке данных** за счет параллелизма и распределенных вычислений. Он поддерживает различные типы вычислений, включая пакетную обработку, интерактивные запросы, потоковую обработку, машинное обучение и графовые вычисления.\n","\n","Spark стал популярным благодаря своей скорости и гибкости. В отличие от традиционных систем обработки данных, таких как Hadoop MapReduce, Spark выполняет вычисления в памяти, что значительно ускоряет обработку данных. Это особенно важно для задач, связанных с машинным обучением, анализом данных и обработкой больших данных, где время выполнения играет критическую роль.\n","\n","Одним из ключевых компонентов Spark является библиотека **MLlib**, которая предоставляет инструменты для машинного обучения на больших данных. Она включает алгоритмы для кластеризации, классификации, регрессии, обработки графов и других задач. Spark также поддерживает интеграцию с Hadoop и может обрабатывать данные из различных источников, таких как HDFS, Cassandra, HBase и S3.\n","\n","У Apache Spark имеется Python API — **PySpark** , который позволяет использовать все возможности Spark непосредственно из Python. Он предоставляет интерфейс для работы с данными на больших масштабах, что делает его популярным среди разработчиков и аналитиков данных, особенно в контексте больших данных и распределенных вычислений. PySpark позволяет обрабатывать огромные объемы данных, распределяя задачи между множеством узлов в кластере. Это делает его особенно полезным для анализа данных в реальном времени, обработки логов, работы с данными из веб-сайтов и других источников больших данных."],"metadata":{"id":"U0g1nyQKF7bk"}},{"cell_type":"markdown","source":["## TensorFlow, Keras"],"metadata":{"id":"ilUOhKsWApOh"}},{"cell_type":"markdown","source":["**TensorFlow**, разработанный Google, представляет собой мощную и гибкую платформу, которая позволяет **создавать и обучать нейросети**. Он поддерживает широкий спектр операций и предоставляет разработчикам возможность создавать собственные архитектуры, компоненты к уже существующим архитектурам и использовать готовые модели. Благодаря своей популярности, TensorFlow имеет огромное сообщество разработчиков и исследователей, что обеспечивает обширную поддержку и множество ресурсов для обучения. Регулярные обновления и улучшения делают его актуальным и конкурентоспособным. TensorFlow также поддерживает множество платформ, включая мобильные устройства, облачные сервисы и распределенные системы, что делает его масштабируемым для различных задач.\n","\n","**Keras**, с другой стороны, является **высокоуровневым API**, который упрощает процесс создания и обучения нейронных сетей. Keras теперь **интегрирован в TensorFlow как tf.keras**, что обеспечивает доступ к мощным низкоуровневым возможностям TensorFlow, сохраняя при этом простоту и удобство использования. Keras предоставляет интуитивно понятный интерфейс, который позволяет разработчикам быстро прототипировать и экспериментировать с моделями. Его модульность и расширяемость позволяют легко комбинировать различные компоненты и создавать сложные архитектуры.\n","\n","Обе библиотеки обладают универсальностью, поддерживая широкий спектр задач глубокого обучения. Они оптимизированы для высокой производительности, что важно для больших и сложных моделей. Обилие документации, учебных материалов и примеров кода облегчает обучение и использование. Регулярные обновления и поддержка от Google и сообщества обеспечивают актуальность и надежность.\n","\n","В частности, TensorFlow и Keras используются для решения задач, таких как классификация изображений, обработка естественного языка, генерация текста, распознавание речи и многое другое. Например, модели, созданные с использованием TensorFlow и Keras, применяются в Google Photos для автоматической классификации изображений, в Google Translate для улучшения качества перевода."],"metadata":{"id":"Ht7ZKpg-gEQb"}},{"cell_type":"markdown","source":["## PyTorch"],"metadata":{"id":"Nt3VDW1_Arxm"}},{"cell_type":"markdown","source":["**PyTorch** — это популярный фреймворк с открытым исходным кодом для глубокого обучения, который был разработан Facebook AI Research (FAIR), конкурирующий по популярности с Tensorflow. Он широко используется для разработки и обучения нейронных сетей благодаря своей гибкости, интуитивно понятному интерфейсу и мощным возможностям для работы с тензорами (многомерными массивами данных).\n","\n","Он обладает мощной поддержкой GPU, что позволяет значительно ускорить обучение моделей, особенно в задачах глубокого обучения, таких как обработка изображений и текста. Фреймворк предлагает удобные инструменты для автоматического вычисления градиентов и оптимизации, что упрощает процесс обучения нейронных сетей.\n","\n","Благодаря своей гибкости, PyTorch стал основным инструментом для исследований в области искусственного интеллекта. Многие научные статьи и исследования в области глубокого обучения используют PyTorch, что способствует его распространению в академических кругах и среди разработчиков. Однако, в отличие от Keras, PyTorch предоставляет тонкую настройку нейросетей, и такая гибкость для новичков может быть излишней, а потому и отталкивать от фреймворка."],"metadata":{"id":"V-8OA7AaFT9R"}},{"cell_type":"markdown","source":["## AutoML-фреймворки\n"],"metadata":{"id":"atcEhyF8fu1s"}},{"cell_type":"markdown","source":["В настоящий момент в мире существуют десятки различных **AutoML**-решений. Одни из них решают задачи общего назначения на табличных данных. Другие заточены на решение более специализированных задач в таких областях как обработка естественного языка или компьютерное зрение, и используют методы глубокого обучения.\n","\n","Под AutoML в узком смысле поднимают обычно автоматизацию задач машинного обучения в части подбора настроечных параметров, так называемых гиперпараметров выбора алгоритмов и архитектур. В более широком смысле под AutoML понимают автоматизацию всех задач, связанных с разработкой модели: от предподготовки данных и отбора переменных до вопросов, связанных с предельной отчетностью, валидацией, дистилляцией моделей, необходимых для их успешного внедрения. Все AutoML делятся на два класса: исследовательский AutoML, который апробирует какие-то новые подходы, и индустриальный AutoML, задача которого — решение с наилучшим качеством за разумное время.\n","\n","Технологии автоматизации машинного обучения открывают много возможностей. Первая, и наиболее важная, — это возможность применения технологий машинного обучения в тех задачах, где ранее затраты на разработку моделей машинного обучения были выше получаемой выгоды от их применения. Помимо крупных задач машинного обучения, где решение одной задачи сразу приносит большую выгоду, есть много задач, отдача каждой из которых сравнительно невелика. Однако, если внимательно посмотреть, в банке таких задач могут быть тысячи, и решение этих задач в совокупности может принести доход не меньше, а может, даже больше, чем решение большого количества сравнительно маржинальных задач.\n","\n","Вторая возможность, которую открывает технология AutoML — задача устранения гандикапа или поддержания актуальности моделей. Для некоторых задач важны не только обилие и разнообразие данных, но и их актуальность, важна возможность принимать решения на самых свежих данных. Например, когда стоит задача проанализировать рекламную кампанию, то уже за месяц очень многое меняется, и период, в течение которого выясняется, откликнулся клиент на кампанию или нет, как правило, составляет от нескольких недель до одного месяца.\n","\n","**AutoML — это новый технологический подход к моделированию**, который позволяет создавать типовые модели машинного обучения гораздо быстрее, чем вручную, с качеством, не хуже, чем у специалиста по анализу данных."],"metadata":{"id":"2ca2SzY4Gt-W"}},{"cell_type":"markdown","source":["### LAMA (Сбер)"],"metadata":{"id":"srdGzaSfcO0U"}},{"cell_type":"markdown","source":["В Лаборатории по ИИ Сбера (Sber AI Lab) создали фреймворк LightAutoML, сокращённо — LAMA.\n","\n","LightAutoML упрощает процесс создания моделей машинного обучения, автоматизируя задачи подготовки данных, выбора признаков (feature engineering), настройки гиперпараметров и построения моделей. Это позволяет пользователям без глубоких знаний в области машинного обучения создавать эффективные модели.\n","\n","Этот AutoML-фреймворк может работать с табличными данными, текстом, временными рядами и изображениями. Это делает инструмент универсальным и применимым в широком спектре задач, от анализа данных до прогнозирования и классификации.\n","\n","LightAutoML является проектом с открытым исходным кодом, что позволяет пользователям свободно использовать, изменять и распространять его."],"metadata":{"id":"4_uEH7jsKKb9"}},{"cell_type":"markdown","source":["# Средства автоматизации и управления экспериментами"],"metadata":{"id":"zwMNe1QtiDbN"}},{"cell_type":"markdown","source":["Помимо того, что модели машинного обучения надо разработать и обучить, **результаты обучения и данные для обучения** (если они как-то менялись) **желательно версионировать** — то есть логгировать полученные показатели, сами модели, артевакты в виде графиков, параметров и прочих вещей. В Excel-таблице это не удержать, а если и удерживать, то с автоматизацией всегда приятнее.\n","\n","MLflow, Weights & Biases (W&B), и DVC (Data Version Control) — это одни из популярных инструментов, которые помогают управлять экспериментами, отслеживать метрики, версионировать данные и модели, а также обеспечивать воспроизводимость и контроль в процессе разработки моделей машинного обучения."],"metadata":{"id":"bTGlB8d1TqiP"}},{"cell_type":"markdown","source":["## MLflow"],"metadata":{"id":"Zk8MawiXiE3c"}},{"cell_type":"markdown","source":["MLflow — это платформа с открытым исходным кодом, которая предназначена для управления жизненным циклом моделей машинного обучения.\n","\n","Пройдемся кратко по функционалу:\n","- сохраняет информацию о каждом запуске модели, включая гиперпараметры, используемые данные, код, и результаты, что облегчает воспроизводимость и анализ.\n","- содержит информацию о зависимостях и командах для запуска моделей локально или из Docker-контейнера.\n","-  сохраняет в формате, который поддерживается различными фреймворками (например, TensorFlow, PyTorch, scikit-learn), и легко загружать для использования в DEV или PROD-средах.\n","- хранить версии моделей, управлять их состояниями (например, черновик, готовая для развертывания) и отслеживать историю изменений."],"metadata":{"id":"WcLjeRYqU9WB"}},{"cell_type":"markdown","source":["## Weights & Biases и ClearML\n","\n","W&B - это собрат MLflow по функционалу, однако ориентирован на глубокое обучение и активно используется в исследовательских и инженерных командах, которые работают над сложными моделями.\n","\n","Из особенностей функционала можно выделить:\n","\n","- W&B интегрируется с инструментами для оптимизации гиперпараметров, что позволяет автоматизировать процесс поиска наилучших настроек модели.\n","- интерактивные графики с метриками моделей (в отличие от статичных метрик в MLflow)\n","- W&B легко интегрируется с такими фреймворками, как TensorFlow, PyTorch, Keras, и другими.\n","\n","Про ClearML можно сказать примерно то же самое. Их ключевое отличие от MLflow — меньшее количество кода или вовсе интеграция из коробки."],"metadata":{"id":"l4H9AoLmiIC4"}},{"cell_type":"markdown","source":["## DVC\n","\n","— это система управления версиями данных, созданная для работы с большими объемами данных в проектах машинного обучения. DVC помогает версионировать и отслеживать изменения не только в коде, но и в данных и моделях, что критически важно для воспроизводимости экспериментов.\n","\n","Его можно считать аналогом Git (так как он построен поверх самого Git), но для данных.\n","\n","- DVC интегрируется с Git, что позволяет управлять версиями данных так же, как и версиями кода. Это упрощает совместную работу над проектами и обеспечивает полную воспроизводимость экспериментов.\n","- DVC поддерживает создание пайплайнов для автоматизации процессов обработки данных, обучения моделей и валидации."],"metadata":{"id":"qr_tLF6eiNCC"}},{"cell_type":"markdown","source":["# Визуализация данных"],"metadata":{"id":"4Yqo10yWiYZ1"}},{"cell_type":"markdown","source":["## Библиотеки seaborn, plotly, matplotlib"],"metadata":{"id":"Ou29oDCIia4V"}},{"cell_type":"markdown","source":["Seaborn, Matplotlib и Plotly — это три популярных библиотеки для визуализации данных в Python. Каждая из них имеет свои особенности и преимущества, и они часто используются в различных задачах, связанных с анализом и представлением данных.\n","\n","**Matplotlib** — это **базовая библиотека** для создания статических, а также иногда интерактивных и анимационных графиков в Python. Это одна из старейших и наиболее широко используемых библиотек для визуализации данных. Matplotlib предоставляет гибкий и мощный API для создания различных типов графиков: от простых линейных графиков до сложных фигур с множеством элементов.\n","\n","Matplotlib часто используется в качестве основы для других библиотек визуализации, таких как Seaborn, которые строятся на ее основе.\n","\n","**Seaborn** предназначена для статистической визуализации, построенная на основе Matplotlib. Она предоставляет удобный интерфейс для создания красивых и информативных статистических графиков с минимальным количеством кода. Seaborn упрощает создание сложных графиков, особенно когда дело касается визуализации распределений и отношений между переменными.\n","\n","**Plotly** — это библиотека для создания интерактивных графиков и визуализаций в Python. В отличие от Matplotlib и Seaborn, Plotly **специализируется на создании интерактивных графиков**, которые можно исследовать и изменять в режиме реального времени. Он позволяет создавать графики с поддержкой масштабирования, наведения на элементы и отображения дополнительных данных при взаимодействии с графиком. В Plotly есть упрощенный API под названием Plotly Express, который позволяет быстро создавать интерактивные графики с минимальным количеством кода - поэтому если и будем использовать plotly, то этот модуль.\n","\n","> **Matplotlib** является отличным выбором, если вам нужна максимальная гибкость и контроль над графиками, особенно если вы создаете сложные фигуры с множеством элементов.\n","\n","> **Seaborn** стоит использовать, когда вам нужно быстро создать красивые и информативные статистические графики. Эта библиотека отлично подходит для анализа данных и презентации результатов.\n","\n","> **Plotly** будет лучшим выбором для создания интерактивных графиков, которые можно использовать в веб-приложениях или демонстрациях, где важна возможность исследования данных в режиме реального времени."],"metadata":{"id":"oJn3Ap7nRTlM"}},{"cell_type":"markdown","source":["## Инструменты: Tableau и MetaBase"],"metadata":{"id":"uXYFGdi2ifvT"}},{"cell_type":"markdown","source":["Tableau и Metabase — это популярные инструменты бизнес-аналитики и визуализации данных. Оба предназначены для того, чтобы помочь пользователям анализировать данные и принимать обоснованные решения, но они имеют различные подходы, функции и целевую аудиторию. Давайте рассмотрим их подробнее.\n","\n","**Tableau** — мощная платформа для визуализации данных, которая позволяет пользователям создавать интерактивные и информативные отчеты и дашборды. Tableau ориентирован на профессионалов в области данных и бизнес-аналитики, предлагая широкий набор инструментов для работы с большими объемами данных и сложными аналитическими задачами.\n","\n","Tableau позволяет создавать **сложные интерактивные дашборды**, которые могут включать в себя разнообразные графики, карты и диаграммы. Пользователи могут взаимодействовать **с данными в реальном времени**, фильтровать и просматривать информацию в различных разрезах. Работа с сервисом идет через интерфейс drag-and-drop, который упрощает процесс создания визуализаций даже для пользователей **без глубоких знаний в области программирования**.\n","Tableau может подключаться к множеству источников данных, включая базы данных, облачные хранилища, файлы Excel, Google Sheets и другие. Он поддерживает как простые табличные данные, так и сложные многомерные наборы данных."],"metadata":{"id":"xwCuDnGhSWx0"}},{"cell_type":"markdown","source":["**Metabase** — бесплатный и с открытым исходным кодом инструмент для бизнес-аналитики, ориентированный на простоту и доступность для конечных пользователей. Metabase предназначен для того, чтобы любой человек в организации мог легко задавать вопросы к данным и получать на них ответы в виде диаграмм и отчетов.\n","\n","Metabase имеет очень простой и интуитивно понятный интерфейс, который позволяет даже пользователям без навыков программирования создавать запросы к данным и строить базовые визуализации. Пользователи могут создавать запросы с помощью конструктора запросов (Query Builder), что делает его доступным для всех уровней пользователей. Для более опытных пользователей Metabase позволяет писать собственные SQL-запросы для получения данных и создания отчетов, что делает его достаточно гибким для сложных аналитических задач.\n","\n","**Tableau ориентирован на профессионалов в области данных** и бизнес-аналитики, предлагая более мощные инструменты для глубокого анализа и визуализации. **Metabase**, напротив, **предназначен для более широкой аудитории**, включая пользователей без технического образования, и фокусируется на простоте использования."],"metadata":{"id":"3UNNi7pJSxzd"}}]}